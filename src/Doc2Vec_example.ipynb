{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec as D2V\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk import download as nltkDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_helper import DBHelper\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hmenn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkDownload('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBHelper()\n",
    "db.connect(user=\"root\",passwd=\"Hasan5695*\",db=\"cse496\")\n",
    "tweets = db.getTweets(\"denemeShort\")\n",
    "tweets.sort(key=lambda t: t.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleSentName: Hayat bu, bir bakarsın herşey bir anda son bulur.\n",
      " Hayat bu, son dediğin an herşey yeniden can bulur..Şemsi Tebrizi\n"
     ]
    }
   ],
   "source": [
    "print(\"ExampleSentName:\",tweets[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopword_set = set(stopwords.words(\"turkish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \" Hiçbir su, zalimlerin yüzkarasını temizleyemez\"\n"
     ]
    }
   ],
   "source": [
    "# clear tweets\n",
    "for tweet in tweets:\n",
    "    tweet.text = re.sub(r\"RT(^|[^@\\w])@(\\w{1,15})\\b:\",\"\",tweet.text)\n",
    "print(tweets[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        dlist = list(set(dlist).difference(stopword_set))\n",
    "        new_data.append(dlist)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = nlp_clean([tweet.text for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedData = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate([tweet.text for tweet in tweets[:500]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(taggedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmenn/Workspace/doc2graph/venv/lib/python3.5/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10\n",
      "iteration 20\n",
      "iteration 30\n",
      "iteration 40\n",
      "iteration 50\n",
      "iteration 60\n",
      "iteration 70\n",
      "iteration 80\n",
      "iteration 90\n",
      "iteration 100\n",
      "iteration 110\n",
      "iteration 120\n",
      "iteration 130\n",
      "iteration 140\n",
      "iteration 150\n",
      "iteration 160\n",
      "iteration 170\n",
      "iteration 180\n",
      "iteration 190\n",
      "iteration 200\n",
      "iteration 210\n",
      "iteration 220\n",
      "iteration 230\n",
      "iteration 240\n",
      "iteration 250\n",
      "iteration 260\n",
      "iteration 270\n",
      "iteration 280\n",
      "iteration 290\n",
      "iteration 300\n",
      "iteration 310\n",
      "iteration 320\n",
      "iteration 330\n",
      "iteration 340\n",
      "iteration 350\n",
      "iteration 360\n",
      "iteration 370\n",
      "iteration 380\n",
      "iteration 390\n",
      "iteration 400\n",
      "iteration 410\n",
      "iteration 420\n",
      "iteration 430\n",
      "iteration 440\n",
      "iteration 450\n",
      "iteration 460\n",
      "iteration 470\n",
      "iteration 480\n",
      "iteration 490\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 500\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = D2V(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(taggedData)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if epoch %10 == 0:\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(taggedData,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"../outputs/d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('430', 0.8913657665252686),\n",
       " ('138', 0.8643275499343872),\n",
       " ('400', 0.8520273566246033),\n",
       " ('58', 0.8425953388214111),\n",
       " ('398', 0.8381558656692505),\n",
       " ('257', 0.8364518284797668),\n",
       " ('196', 0.835493803024292),\n",
       " ('156', 0.8344122171401978),\n",
       " ('223', 0.8268957138061523),\n",
       " ('433', 0.8143468499183655)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kibir, bele bağlanmış taş gibidir. Onunla ne yüzülür ne de uçulur. (Hacı Bayram-ı Veli)\n",
      "430 Ne güzel ümmetin olmak..Ne güzel seni görmeden sevmek Ya Resulallah (S.A.V.) Kandilimiz mübarek olsun.. http://t.co/KJg7HYrjJ6\n",
      "58  Gel de birbirimizin kıymetini bilelim. Çünkü ansızın ayrılacağız birbirimizden.  // Mevlana\n",
      "61 Önce Trabzonspor,  sonra Fenerbahçe şimdi ise Beşiktaş ! haydı hayırlısı...\n",
      "Şaka gibi...\n"
     ]
    }
   ],
   "source": [
    "print(1,tweets[1].text)\n",
    "print(430,tweets[430].text)\n",
    "print(58,tweets[58].text)\n",
    "print(61,tweets[61].text) # <- BURASI FARKLI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
