{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec as D2V\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk import download as nltkDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_helper import DBHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hmenn/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkDownload('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBHelper()\n",
    "db.connect(user=\"root\",passwd=\"Hasan5695*\",db=\"cse496\")\n",
    "tweets = db.getTweets(\"denemeShort\")\n",
    "tweets.sort(key=lambda t: t.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleSentName: Hayat bu, bir bakarsın herşey bir anda son bulur.\n",
      " Hayat bu, son dediğin an herşey yeniden can bulur..Şemsi Tebrizi\n"
     ]
    }
   ],
   "source": [
    "print(\"ExampleSentName:\",tweets[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopword_set = set(stopwords.words(\"turkish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        dlist = list(set(dlist).difference(stopword_set))\n",
    "        new_data.append(dlist)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = nlp_clean([tweet.text for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedData = [TaggedDocument(words=word_tokenize(_d.lower()), tags=str(i)) for i, _d in enumerate([tweet.text for tweet in tweets])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['hayat', 'bu', ',', 'bir', 'bakarsın', 'herşey', 'bir', 'anda', 'son', 'bulur', '.', 'hayat', 'bu', ',', 'son', 'dediğin', 'an', 'herşey', 'yeniden', 'can', 'bulur..şemsi', 'tebrizi'], 0)\n"
     ]
    }
   ],
   "source": [
    "print(taggedData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmenn/Workspace/doc2graph/venv/lib/python3.5/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = D2V(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(taggedData)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(taggedData,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"../outputs/d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7', 0.9380130171775818),\n",
       " ('0', 0.7842957973480225),\n",
       " ('9', 0.7634994983673096),\n",
       " ('6', 0.7404583692550659),\n",
       " ('3', 0.703612208366394),\n",
       " ('2', 0.6429765224456787),\n",
       " ('8', 0.5338811874389648),\n",
       " ('4', 0.4151258170604706),\n",
       " ('5', 0.23789331316947937)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kibir, bele bağlanmış taş gibidir. Onunla ne yüzülür ne de uçulur. (Hacı Bayram-ı Veli)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hayat bu, bir bakarsın herşey bir anda son bulur.\\n Hayat bu, son dediğin an herşey yeniden can bulur..Şemsi Tebrizi'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
