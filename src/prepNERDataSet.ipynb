{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file converts TWNERTC dataset to CoNLL format.\n",
    "*Each sentence splitten into words and each words has it's entity on the same line* <br/>\n",
    "\n",
    "You can download TWNERTC dataset from official source with the link below\n",
    "\n",
    "[TWNERTC_DATASET_DOWNLOAD](https://data.mendeley.com/datasets/cdcztymf4k/1)\n",
    "\n",
    "** Work Example**<br/>\n",
    "\n",
    "*TWNERTC_INPUT*<br/>\n",
    "location B-PERSON B-LOCATION 0 Hasan Istanbul'a gitti.\n",
    "\n",
    "*CoNLL_OUTPUT*<br/>\n",
    "Hasan B-PER<br/>\n",
    "Istanbul'a B-LOC<br/>\n",
    "gitti 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH='/home/hmenn/Workspace/CSE496_Node2Vec_2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut type names\n",
    "def convertType(t):\n",
    "    if t==\"B-PERSON\":\n",
    "        return \"B-PER\"\n",
    "    elif t==\"I-PERSON\":\n",
    "        return \"I-PER\"\n",
    "    elif t==\"B-LOCATION\":\n",
    "        return \"B-LOC\"\n",
    "    elif t==\"I-LOCATION\":\n",
    "        return \"I-LOC\"\n",
    "    elif t==\"B-ORGANIZATION\":\n",
    "        return \"B-ORG\"\n",
    "    elif t==\"I-ORGANIZATION\":\n",
    "        return \"I-ORG\"\n",
    "    else:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data according to required size.\n",
    "def split_data(inFile, outputFilePath, split_size):\n",
    "    '''\n",
    "        inFile: file descriptor which come from 'with open(...) as inFile'\n",
    "    '''\n",
    "    c = 0\n",
    "    with open(outputFilePath,\"w\") as outFile:\n",
    "        line = inFile.readline()\n",
    "        while line and c != split_size:\n",
    "            splits = line.split()\n",
    "            mid = int(len(splits[1:])/2)\n",
    "            for i,j in zip(splits[1:mid],splits[mid+1:]):\n",
    "                i = convertType(i)\n",
    "                str=j+\"\\t\"+i+\"\\n\"\n",
    "                outFile.write(str)\n",
    "            outFile.write(\"\\n\")\n",
    "            line = inFile.readline()\n",
    "            c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFilePath=PROJECT_PATH + '/dataset/TWNERTC/TWNERTC_TC_Coarse Grained NER_No_NoiseReduction.DUMP'\n",
    "outputDir = PROJECT_PATH + '/outputs/NER_dataset'\n",
    "\n",
    "trainFilePath=outputDir + '/train.txt'\n",
    "testaFilePath=outputDir + '/dev.txt'\n",
    "testbFilePath=outputDir + '/test.txt'\n",
    "\n",
    "train_size = 100000\n",
    "testa_size = 10000\n",
    "testb_size = 10000\n",
    "\n",
    "# Create output directory if not exist\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Convertion\n",
    "with open(inputFilePath,\"r\") as inFile:\n",
    "    split_data(inFile, trainFilePath, train_size)\n",
    "    split_data(inFile, testaFilePath, testa_size)\n",
    "    split_data(inFile, testbFilePath, testb_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
